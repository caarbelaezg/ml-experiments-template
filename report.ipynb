{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "involved-declaration",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "In this notebook we report progress on the project of house price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aerial-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "confidential-gender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "legal-botswana",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "successful-vegetable",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "statutory-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(*, model, metric, X_train, y_train, X_test, y_test):\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "    train_error = metric(y_train, train_predictions)\n",
    "    test_error = metric(y_test, test_predictions)\n",
    "    return {\n",
    "        \"train_predictions\": train_predictions,\n",
    "        \"test_predictions\": test_predictions,\n",
    "        \"train_error\": train_error,\n",
    "        \"test_error\": test_error\n",
    "    }\n",
    "\n",
    "def print_report(*, model, evaluation):\n",
    "    print(f\"Model used:\\n\\t{reg}\")\n",
    "    print(f\"Error:\\n\\ttrain set {evaluation['train_error']}\\n\\ttest error: {evaluation['test_error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "hazardous-profile",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "union-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "central-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data.get_dataset(\n",
    "    partial(pd.read_csv, filepath_or_buffer=dataset_path),\n",
    "    splits=(\"train\", \"test\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boring-pastor",
   "metadata": {},
   "source": [
    "**If you need to visualize anything from your training data, do it here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-speed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-throw",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "specific-solid",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "Before doing any complex Machine Learning model, let's try to solve the problem by having an initial educated guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "short-comparative",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('average-price-per-neighborhood-regressor',\n",
      "                 AveragePricePerNeighborhoodRegressor())])\n",
      "Error:\n",
      "\ttrain set 33609.9990756996\n",
      "\ttest error: 34799.09487677742\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-04 15-12\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-hardwood",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "indian-senate",
   "metadata": {},
   "source": [
    "## Linear Regression Model \n",
    "\n",
    "We want to try easy things first, so know lets see how a linear regression model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "pending-choir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge'],\n",
      "                                    force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 10918.49482187895\n",
      "\ttest error: 10603307471585.064\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 02-46\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-vector",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort and discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sixth-terrace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-description",
   "metadata": {},
   "source": [
    "## Linear regression with Feature Engineering\n",
    "\n",
    "Probably the previous model is not good enough, let's see how is the performance of a model using some produced features.\n",
    "\n",
    "Techniques:\n",
    "1. Feature Cross\n",
    "2. Discretizer\n",
    "3. Add average per neighborhood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "deluxe-lesbian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('averager', AveragePricePerNeighborhoodExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge',\n",
      "                                                                     'AveragePriceInNeihborhood'],\n",
      "                                    force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('linear-regressor', LinearRegression())])\n",
      "Error:\n",
      "\ttrain set 10956.657887374604\n",
      "\ttest error: 17232262514823.115\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 02-49\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-receipt",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-merchandise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "opened-borough",
   "metadata": {},
   "source": [
    "## Regularized Linear Regression\n",
    "\n",
    "Let's assume you are overfitting. Load the results of a linear regression model with regularized loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "lesbian-intellectual",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:\n",
      "\tPipeline(steps=[('age-extractor', AgeExtractor()),\n",
      "                ('categorical-encoder',\n",
      "                 CategoricalEncoder(additional_pass_through_columns=['HouseAge',\n",
      "                                                                     'RemodAddAge',\n",
      "                                                                     'GarageAge'],\n",
      "                                    force_dense_array=True, one_hot=True)),\n",
      "                ('standard-scaler', StandardScaler()),\n",
      "                ('ridge-regressor', Ridge(alpha=100))])\n",
      "Error:\n",
      "\ttrain set 11278.049274904646\n",
      "\ttest error: 17979.97582848677\n"
     ]
    }
   ],
   "source": [
    "model_path = os.path.join(\"models\", \"2021-06-05 02-43\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educated-korean",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-science",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hidden-begin",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "Decision trees ofer great complexity, they can fit even a noisy dataset almost perfectly. Let's see how it behaves on the task at hand. \n",
    "\n",
    "**Overfiting case**\n",
    "Let's see the results for a model that has greatly overfit the data, this wouldn't be an ideal model, but at least it could tell that our model is powerful enough for the task at hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "theoretical-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-ecuador",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-wedding",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "blessed-pattern",
   "metadata": {},
   "source": [
    "**Using best hyper params** Now let's see thow much a simple decision tree can give us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-appendix",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-order",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "assigned-acting",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Now it is time to use a model that can properly help us to regularize the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"models\", \"\", \"model.joblib\")\n",
    "reg = joblib.load(model_path)\n",
    "evaluation = evaluate_model(\n",
    "    model=reg,\n",
    "    metric=metrics.custom_error,\n",
    "    X_train=dataset[\"train\"][0],\n",
    "    y_train=dataset[\"train\"][1],\n",
    "    X_test=dataset[\"test\"][0],\n",
    "    y_test=dataset[\"test\"][1]\n",
    ")\n",
    "print_report(model=reg, evaluation=evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-badge",
   "metadata": {},
   "source": [
    "**Error Analysis**\n",
    "\n",
    "What can you learn about the errors your model is making? Try this:\n",
    "\n",
    "* Discretize the errors your model is making by some categorical variables.\n",
    "* Sort or discretize the errors your model is making and see what the features have in common in those cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-profession",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-theology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
